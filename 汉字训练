import os
import numpy as np
from PIL import Image
import cv2
from sklearn.utils import shuffle
import tensorflow as tf
file_dir="F:/BaiduNetdiskDownload/2020AIdata/tmp/"
def resize_image(img):
    # 补方
    pad_size = abs(img.shape[0] - img.shape[1]) // 2
    if img.shape[0] < img.shape[1]:
        pad_dims = ((pad_size, pad_size), (0, 0))
    else:
        pad_dims = ((0, 0), (pad_size, pad_size))
    img = np.lib.pad(img, pad_dims, mode='constant', constant_values=255)
    # 缩放
    img = cv2.resize(img, (64 - 4 * 2, 64 - 4 * 2))
    img = np.lib.pad(img, ((4, 4), (4, 4)), mode='constant', constant_values=255)
    assert img.shape == (64, 64)
    total=0
    for i in range(4,img.shape[0]-4):
        for j in range(4,img.shape[1]-4):
            total=total+img[i][j]
    #算出平均值
    total=total/(img.shape[0]*img.shape[1])
    for i in range(0,img.shape[0]):
        for j in range(0,img.shape[1]):
            if img[i][j]<total:
                img[i][j]=-1
            else:
                img[i][j]=1
    #小于该值变为黑色 大于该值变为红色
    img = img.flatten()
    # 像素值范围-1到1
    return img
def get_file(file_dir):
    dict={}
    A=[]
    HHH=0
    for file in os.listdir(file_dir):
        A.append(file_dir+file)
    i=0
    j=0
    now="一"
    dict[i]=now
    img=[]
    label=[]
    img_text = []
    label_text = []
    for files in A:
        for tem in os.listdir(files):
            name=files.split(sep='/')
            filename=files+'/'+tem
            print(filename)
            if (name[4] == now):
                if (j < 150):
                    img.append(filename)
                    label.append(i)
                    j = j + 1
                else:
                    img_text.append(filename)
                    label_text.append(i)
                    j = j + 1
            else:
                j = 0
                i = i + 1
                now = name[4]
                dict[i] = now
                img.append(filename)
                label.append(i)
    print(dict)
    train_data_x, train_data_y = shuffle(img, label, random_state=0)
    text_data_x, text_data_y = shuffle(img_text, label_text, random_state=0)
    return train_data_x, train_data_y, text_data_x, text_data_y, dict

# def convert_to_one_hot(char):
#     vector = np.zeros(3755)
#     vector[char_set.index(char)] = 1
#     return vector
train_data_x, train_data_y, text_data_x, text_data_y, dict=get_file(file_dir)
X = tf.placeholder(tf.float32, [None, 64 * 64], name='x')
Y = tf.placeholder(tf.float32, [None, 3755], name="y")
keep_prob = tf.placeholder(tf.float32,name='prob')
batch_size = 128
num_batch = len(train_data_x) // batch_size
train_batch_x = []
train_batch_y = []
for i in range(num_batch):
    print(i)
    batch_x = []
    batch_y = np.zeros((128, 3755),dtype='float32')
    for j in range(128 * i, 128 * (i + 1)):
        image11 = cv2.imdecode(np.fromfile(train_data_x[j], dtype=np.uint8), -1)
        img_gray = cv2.cvtColor(image11, cv2.COLOR_RGB2GRAY)
        imhhh = resize_image(img_gray)
        batch_x.append(imhhh)
        batch_y[j%128][train_data_y[j]] = 1
    train_batch_x.append(batch_x)
    train_batch_y.append(batch_y)
def chinese_hand_write_cnn():
    x = tf.reshape(X, shape=[-1, 64, 64, 1])
    # 2 conv layers
    w_c1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01),name='w_c1')
    b_c1 = tf.Variable(tf.zeros([32]),name='b_c1')
    conv1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, w_c1, strides=[1, 1, 1, 1], padding='SAME'), b_c1))
    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

    w_c2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01),name='w_c2')
    b_c2 = tf.Variable(tf.zeros([64]),name='b_c2')
    conv2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv1, w_c2, strides=[1, 1, 1, 1], padding='SAME'), b_c2))
    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    """
    # 可以增加一层网络
    w_c3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))
    b_c3 = tf.Variable(tf.zeros([128]))
    conv3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv2, w_c3, strides=[1, 1, 1, 1], padding='SAME'), b_c3))
    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    conv3 = tf.nn.dropout(conv3, keep_prob)
    """
 # 全连接层，8*32*64
    w_d = tf.Variable(tf.random_normal([8 * 32 * 64, 1024], stddev=0.01),name='w_d')
    b_d = tf.Variable(tf.zeros([1024]),name='b_d')
    dense = tf.reshape(conv2, [-1, w_d.get_shape().as_list()[0]])
    dense = tf.nn.relu(tf.add(tf.matmul(dense, w_d), b_d))
    dense = tf.nn.dropout(dense, keep_prob)

    w_out = tf.Variable(tf.random_normal([1024, 3755], stddev=0.01),name='w_out')
    b_out = tf.Variable(tf.zeros([3755]),name='b_out')
    out = tf.add(tf.matmul(dense, w_out), b_out,name="out")

    return out


def train_hand_write_cnn():
    output = chinese_hand_write_cnn()
    cross_entropy = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=output))
    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(Y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    saver = tf.train.Saver(max_to_keep=1)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for e in range(16):
            for i in range(num_batch):
                batch_x1 = train_batch_x[i]
                batch_y1 = train_batch_y[i]
                train_step.run(feed_dict={X: batch_x1, Y: batch_y1, keep_prob: 0.5})
            train_accuracy = accuracy.eval(feed_dict={
                X: batch_x, Y: batch_y, keep_prob: 1.0
            })
            print("step %d,training accuracy %g" % (e, train_accuracy))
            saver.save(sess, 'my_net1/my_test_model', global_step=e)



train_hand_write_cnn()

